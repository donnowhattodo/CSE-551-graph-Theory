{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9965771,"sourceType":"datasetVersion","datasetId":5999182}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-23T19:02:01.378689Z","iopub.execute_input":"2024-11-23T19:02:01.379110Z","iopub.status.idle":"2024-11-23T19:02:01.860632Z","shell.execute_reply.started":"2024-11-23T19:02:01.379073Z","shell.execute_reply":"2024-11-23T19:02:01.859258Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/cse551/Youtube_communities.txt\n/kaggle/input/cse551/Collin_CYC_Graph.txt\n/kaggle/input/cse551/CYC2008_complexes.txt\n/kaggle/input/cse551/roadNet_Texas.txt\n/kaggle/input/cse551/Undirected_Youtube_network.txt\n/kaggle/input/cse551/Email-Enron.txt\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install hypernetx matplotlib scikit-learn\n!pip install networkx numpy scikit-learn matplotlib\n!pip install python-louvain","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:22:48.569433Z","iopub.execute_input":"2024-11-23T20:22:48.569910Z","iopub.status.idle":"2024-11-23T20:23:21.738072Z","shell.execute_reply.started":"2024-11-23T20:22:48.569855Z","shell.execute_reply":"2024-11-23T20:23:21.736098Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: hypernetx in /opt/conda/lib/python3.10/site-packages (2.3.8)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.5.2)\nRequirement already satisfied: celluloid>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from hypernetx) (0.2.0)\nRequirement already satisfied: decorator>=5.1.1 in /opt/conda/lib/python3.10/site-packages (from hypernetx) (5.1.1)\nRequirement already satisfied: igraph>=0.11.4 in /opt/conda/lib/python3.10/site-packages (from hypernetx) (0.11.6)\nRequirement already satisfied: networkx>=3.3 in /opt/conda/lib/python3.10/site-packages (from hypernetx) (3.3)\nRequirement already satisfied: pandas>=2.2.2 in /opt/conda/lib/python3.10/site-packages (from hypernetx) (2.2.3)\nRequirement already satisfied: scipy>=1.13 in /opt/conda/lib/python3.10/site-packages (from hypernetx) (1.14.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: texttable>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from igraph>=0.11.4->hypernetx) (1.7.0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.2.2->hypernetx) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.2.2->hypernetx) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (3.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.5.2)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (3.7.5)\nRequirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\nRequirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (1.4.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\nRequirement already satisfied: python-louvain in /opt/conda/lib/python3.10/site-packages (0.16)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from python-louvain) (3.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from python-louvain) (1.26.4)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import networkx as nx\nimport hypernetx as hnx\nimport numpy as np\nfrom collections import defaultdict\nimport community as community_louvain\nfrom sklearn.metrics import (\n    normalized_mutual_info_score,\n    adjusted_rand_score,\n    homogeneity_score,\n    completeness_score,\n    v_measure_score,\n    precision_score,\n    recall_score,\n    f1_score,\n)\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Step 1: Load Graph and Hypergraph\ndef load_network_data(filename, max_edges=900000):\n    G = nx.Graph()\n    hyperedges = defaultdict(set)\n\n    with open(filename, 'r') as file:\n        for i, line in enumerate(file):\n            if i >= max_edges:\n                break\n            if not line.startswith('#'):\n                nodes = list(map(int, line.strip().split()))\n                for node in nodes[1:]:\n                    G.add_edge(nodes[0], node)\n                    hyperedges[node].add(nodes[0])\n\n    # Convert to hypergraph\n    H = hnx.Hypergraph(hyperedges)\n    return G, H\n\n\n# Step 2: Convert Hypergraph to Weighted Graph\ndef hypergraph_to_weighted_graph(H):\n    G_weighted = nx.Graph()\n    for edge, nodes in H.incidence_dict.items():\n        nodes = list(nodes)\n        for i, u in enumerate(nodes):\n            for v in nodes[i + 1:]:\n                if G_weighted.has_edge(u, v):\n                    G_weighted[u][v]['weight'] += 1\n                else:\n                    G_weighted.add_edge(u, v, weight=1)\n    return G_weighted\n\n\n# Step 3: Graph Community Detection\ndef graph_community_detection(G):\n    return community_louvain.best_partition(G)\n\n\n# Step 4: Hypergraph Community Detection (Louvain)\ndef hypergraph_community_detection(H):\n    G_weighted = hypergraph_to_weighted_graph(H)\n    if G_weighted.number_of_edges() == 0:\n        print(\"No edges in the weighted graph derived from hypergraph. Cannot detect communities.\")\n        return {}\n\n    partition = community_louvain.best_partition(G_weighted, weight='weight')\n    return partition\n\n\n# Step 5: Extract Communities\ndef extract_communities(partition):\n    communities = defaultdict(set)\n    for node, community in partition.items():\n        communities[community].add(node)\n    return communities\n\n\n# Step 6: Find Common Nodes\ndef find_common_nodes(graph_partition, hypergraph_partition):\n    return {node for node in graph_partition if node in hypergraph_partition}\n\n\n# Step 7: Compute Clustering Metrics\ndef evaluate_clustering_metrics(graph_partition, hypergraph_partition):\n    common_nodes = find_common_nodes(graph_partition, hypergraph_partition)\n    if not common_nodes:\n        print(\"No common nodes between graph and hypergraph.\")\n        return\n\n    true_labels = [graph_partition[node] for node in common_nodes]\n    pred_labels = [hypergraph_partition[node] for node in common_nodes]\n\n    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n    ari = adjusted_rand_score(true_labels, pred_labels)\n    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n    homogeneity = homogeneity_score(true_labels, pred_labels)\n    completeness = completeness_score(true_labels, pred_labels)\n    v_measure = v_measure_score(true_labels, pred_labels)\n    jaccard = len(set(true_labels).intersection(set(pred_labels))) / len(set(true_labels).union(set(pred_labels)))\n\n    purity = sum(np.max(np.bincount([true, pred])) for true, pred in zip(true_labels, pred_labels)) / len(common_nodes)\n\n    print(\"\\nEvaluation Metrics:\")\n    print(f\"NMI: {nmi:.4f}\")\n    print(f\"ARI: {ari:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Jaccard Similarity Index: {jaccard:.4f}\")\n    #print(f\"Purity: {purity:.4f}\")\n    print(f\"Homogeneity: {homogeneity:.4f}\")\n    print(f\"Completeness: {completeness:.4f}\")\n    print(f\"V-measure: {v_measure:.4f}\")\n\n\n# Step 8: Print Communities\ndef print_communities(communities, label=\"Community\"):\n    print(f\"\\n{label}s:\")\n    for community_id, nodes in communities.items():\n        print(f\"{label} {community_id}: size: {len(nodes)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Main Function\ndef main():\n    filename = '/kaggle/input/cse551/roadNet_Texas.txt'\n\n    # Step 1: Load Network Data\n    G, H = load_network_data(filename)\n    print(f\"Graph loaded with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n    print(f\"Hypergraph loaded with {len(H.edges)} hyperedges.\")\n\n    # Step 2: Graph Community Detection\n    graph_partition = graph_community_detection(G)\n    graph_comm = extract_communities(graph_partition)\n    print(f\"\\nTotal Graph Communities: {len(graph_comm)}\")\n    #print_communities(graph_comm, label=\"Graph Community\")\n\n    # Step 3: Hypergraph Community Detection\n    hypergraph_partition = hypergraph_community_detection(H)\n    if not hypergraph_partition:\n        print(\"No hypergraph communities detected.\")\n        return\n\n    hypergraph_comm = extract_communities(hypergraph_partition)\n    print(f\"\\nTotal Hypergraph Communities: {len(hypergraph_comm)}\")\n    #print_communities(hypergraph_comm, label=\"Hypergraph Community\")\n\n    # Step 4: Compare Communities\n    print(\"\\nComparing Graph and Hypergraph Communities:\")\n    for graph_id, graph_nodes in graph_comm.items():\n        for hyper_id, hyper_nodes in hypergraph_comm.items():\n            overlap = len(graph_nodes & hyper_nodes)\n            #if overlap > 0:\n                #print(f\"Graph Community {graph_id} overlaps with Hypergraph Community {hyper_id}: {overlap} shared nodes\")\n\n    # Step 5: Evaluate Clustering Metrics\n    evaluate_clustering_metrics(graph_partition, hypergraph_partition)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:23:40.220794Z","iopub.execute_input":"2024-11-23T20:23:40.221253Z","iopub.status.idle":"2024-11-23T20:30:43.664564Z","shell.execute_reply.started":"2024-11-23T20:23:40.221213Z","shell.execute_reply":"2024-11-23T20:30:43.663346Z"}},"outputs":[{"name":"stdout","text":"Graph loaded with 326860 nodes and 452105 edges.\nHypergraph loaded with 326860 hyperedges.\n\nTotal Graph Communities: 318\n\nTotal Hypergraph Communities: 167\n\nComparing Graph and Hypergraph Communities:\n\nEvaluation Metrics:\nNMI: 0.8743\nARI: 0.5895\nPrecision: 0.0140\nRecall: 0.0163\nF1 Score: 0.0150\nJaccard Similarity Index: 0.4647\nHomogeneity: 0.8375\nCompleteness: 0.9144\nV-measure: 0.8743\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import networkx as nx\nimport hypernetx as hnx\nimport numpy as np\nfrom collections import defaultdict\nimport community as community_louvain\nfrom sklearn.metrics import (\n    normalized_mutual_info_score,\n    adjusted_rand_score,\n    homogeneity_score,\n    completeness_score,\n    v_measure_score,\n    precision_score,\n    recall_score,\n    f1_score,\n)\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Load Graph and Hypergraph\ndef load_network_data(filename):\n    G = nx.Graph()\n    hyperedges = defaultdict(set)\n\n    with open(filename, 'r') as file:\n        for i, line in enumerate(file):\n            if not line.startswith('#'):\n                nodes = list(map(int, line.strip().split()))\n                for node in nodes[1:]:\n                    G.add_edge(nodes[0], node)\n                    hyperedges[node].add(nodes[0])\n\n    # Convert to hypergraph\n    H = hnx.Hypergraph(hyperedges)\n    return G, H\n\n\n# Step 2: Convert Hypergraph to Weighted Graph\ndef hypergraph_to_weighted_graph(H):\n    G_weighted = nx.Graph()\n    for edge, nodes in H.incidence_dict.items():\n        nodes = list(nodes)\n        for i, u in enumerate(nodes):\n            for v in nodes[i + 1:]:\n                if G_weighted.has_edge(u, v):\n                    G_weighted[u][v]['weight'] += 1\n                else:\n                    G_weighted.add_edge(u, v, weight=1)\n    return G_weighted\n\n\n# Step 3: Graph Community Detection\ndef graph_community_detection(G):\n    return community_louvain.best_partition(G)\n\n\n# Step 4: Hypergraph Community Detection (Louvain)\ndef hypergraph_community_detection(H):\n    G_weighted = hypergraph_to_weighted_graph(H)\n    if G_weighted.number_of_edges() == 0:\n        print(\"No edges in the weighted graph derived from hypergraph. Cannot detect communities.\")\n        return {}\n\n    partition = community_louvain.best_partition(G_weighted, weight='weight')\n    return partition\n\n\n# Step 5: Extract Communities\ndef extract_communities(partition):\n    communities = defaultdict(set)\n    for node, community in partition.items():\n        communities[community].add(node)\n    return communities\n\n\n# Step 6: Find Common Nodes\ndef find_common_nodes(graph_partition, hypergraph_partition):\n    return {node for node in graph_partition if node in hypergraph_partition}\n\n\n# Step 7: Compute Clustering Metrics\ndef evaluate_clustering_metrics(graph_partition, hypergraph_partition):\n    common_nodes = find_common_nodes(graph_partition, hypergraph_partition)\n    if not common_nodes:\n        print(\"No common nodes between graph and hypergraph.\")\n        return\n\n    true_labels = [graph_partition[node] for node in common_nodes]\n    pred_labels = [hypergraph_partition[node] for node in common_nodes]\n\n    nmi = normalized_mutual_info_score(true_labels, pred_labels)\n    ari = adjusted_rand_score(true_labels, pred_labels)\n    precision = precision_score(true_labels, pred_labels, average='weighted', zero_division=0)\n    recall = recall_score(true_labels, pred_labels, average='weighted', zero_division=0)\n    f1 = f1_score(true_labels, pred_labels, average='weighted', zero_division=0)\n    homogeneity = homogeneity_score(true_labels, pred_labels)\n    completeness = completeness_score(true_labels, pred_labels)\n    v_measure = v_measure_score(true_labels, pred_labels)\n    jaccard = len(set(true_labels).intersection(set(pred_labels))) / len(set(true_labels).union(set(pred_labels)))\n\n    purity = sum(np.max(np.bincount([true, pred])) for true, pred in zip(true_labels, pred_labels)) / len(common_nodes)\n\n    print(\"\\nEvaluation Metrics:\")\n    print(f\"NMI: {nmi:.4f}\")\n    print(f\"ARI: {ari:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(f\"Jaccard Similarity Index: {jaccard:.4f}\")\n    #print(f\"Purity: {purity:.4f}\")\n    print(f\"Homogeneity: {homogeneity:.4f}\")\n    print(f\"Completeness: {completeness:.4f}\")\n    print(f\"V-measure: {v_measure:.4f}\")\n\n\n# Step 8: Print Communities\ndef print_communities(communities, label=\"Community\"):\n    print(f\"\\n{label}s:\")\n    for community_id, nodes in communities.items():\n        print(f\"{label} {community_id}: size: {len(nodes)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Function\ndef main():\n    filename = '/kaggle/input/cse551/roadNet_Texas.txt'\n\n    # Step 1: Load Network Data\n    G, H = load_network_data(filename)\n    print(f\"Graph loaded with {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n    print(f\"Hypergraph loaded with {len(H.edges)} hyperedges.\")\n\n    # Step 2: Graph Community Detection\n    graph_partition = graph_community_detection(G)\n    graph_comm = extract_communities(graph_partition)\n    print(f\"\\nTotal Graph Communities: {len(graph_comm)}\")\n    #print_communities(graph_comm, label=\"Graph Community\")\n\n    # Step 3: Hypergraph Community Detection\n    hypergraph_partition = hypergraph_community_detection(H)\n    if not hypergraph_partition:\n        print(\"No hypergraph communities detected.\")\n        return\n\n    hypergraph_comm = extract_communities(hypergraph_partition)\n    print(f\"\\nTotal Hypergraph Communities: {len(hypergraph_comm)}\")\n    #print_communities(hypergraph_comm, label=\"Hypergraph Community\")\n\n    # Step 4: Compare Communities\n    print(\"\\nComparing Graph and Hypergraph Communities:\")\n    for graph_id, graph_nodes in graph_comm.items():\n        for hyper_id, hyper_nodes in hypergraph_comm.items():\n            overlap = len(graph_nodes & hyper_nodes)\n            #if overlap > 0:\n                #print(f\"Graph Community {graph_id} overlaps with Hypergraph Community {hyper_id}: {overlap} shared nodes\")\n\n    # Step 5: Evaluate Clustering Metrics\n    evaluate_clustering_metrics(graph_partition, hypergraph_partition)\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-23T20:35:39.687790Z","iopub.execute_input":"2024-11-23T20:35:39.688735Z","iopub.status.idle":"2024-11-23T21:19:04.244686Z","shell.execute_reply.started":"2024-11-23T20:35:39.688679Z","shell.execute_reply":"2024-11-23T21:19:04.243422Z"}},"outputs":[{"name":"stdout","text":"Graph loaded with 1379917 nodes and 1921660 edges.\nHypergraph loaded with 1379917 hyperedges.\n\nTotal Graph Communities: 751\n\nTotal Hypergraph Communities: 313\n\nComparing Graph and Hypergraph Communities:\n\nEvaluation Metrics:\nNMI: 0.8679\nARI: 0.5520\nPrecision: 0.0135\nRecall: 0.0148\nF1 Score: 0.0139\nJaccard Similarity Index: 0.3808\nHomogeneity: 0.8345\nCompleteness: 0.9040\nV-measure: 0.8679\n","output_type":"stream"}],"execution_count":15}]}